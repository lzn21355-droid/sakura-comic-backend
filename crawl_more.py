import sys
import os
import requests
from requests.exceptions import RequestException
import time

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.factory import create_app
from app.task.tasks import SakuraData
from app.extensions import db
from app.models.models import MovType

class SakuraCrawler:
    def __init__(self):
        self.app = create_app()
        
    def init_mov_types(self):
        """åˆå§‹åŒ–è§†é¢‘ç±»å‹"""
        with self.app.app_context():
            try:
                existing_types = MovType.query.first()
                if existing_types:
                    print("âœ… è§†é¢‘ç±»å‹å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
                    return True
                else:
                    print("ğŸ”„ åˆå§‹åŒ–è§†é¢‘ç±»å‹...")
                    sd = SakuraData()
                    sd.insert_mov_type()
                    print("âœ… è§†é¢‘ç±»å‹åˆå§‹åŒ–å®Œæˆ")
                    return True
            except Exception as e:
                print(f"âŒ è§†é¢‘ç±»å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return False
    
    def crawl_pages(self, start_page=1, end_page=5, max_retries=3, delay=2):
        """çˆ¬å–æŒ‡å®šèŒƒå›´çš„é¡µé¢"""
        with self.app.app_context():
            print(f"ğŸš€ å¼€å§‹çˆ¬å–ç¬¬ {start_page} åˆ° {end_page} é¡µæ•°æ®")
            
            success_count = 0
            fail_count = 0
            
            for page in range(start_page, end_page + 1):
                for attempt in range(max_retries):
                    try:
                        print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ (å°è¯• {attempt + 1}/{max_retries})...")
                        sd = SakuraData()
                        sd.get_mov_detail(page)
                        print(f"âœ… ç¬¬ {page} é¡µçˆ¬å–å®Œæˆ")
                        success_count += 1
                        break
                    except RequestException as e:
                        print(f"âŒ ç¬¬ {page} é¡µç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
                        if attempt < max_retries - 1:
                            print(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                            time.sleep(delay)
                        else:
                            print(f"ğŸ’¥ ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥")
                            fail_count += 1
                    except Exception as e:
                        db.session.rollback()
                        print(f"âŒ ç¬¬ {page} é¡µå¤„ç†å¤±è´¥: {str(e)}")
                        if attempt < max_retries - 1:
                            print(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                            time.sleep(delay)
                        else:
                            print(f"ğŸ’¥ ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥")
                            fail_count += 1
            
            print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æˆåŠŸ: {success_count} é¡µ, å¤±è´¥: {fail_count} é¡µ")
    
    def full_crawl(self):
        """å…¨é‡çˆ¬å–"""
        with self.app.app_context():
            print("ğŸš€ å¼€å§‹å…¨é‡çˆ¬å–...")
            sd = SakuraData()
            sd.crawl_mov_detail_all()
            print("ğŸ‰ å…¨é‡çˆ¬å–å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    crawler = SakuraCrawler()
    
    # åˆå§‹åŒ–è§†é¢‘ç±»å‹
    if crawler.init_mov_types():
        # é€‰æ‹©çˆ¬å–æ–¹å¼
        print("è¯·é€‰æ‹©çˆ¬å–æ–¹å¼:")
        print("1. æµ‹è¯•çˆ¬å– (5é¡µ)")
        print("2. å…¨é‡çˆ¬å–")
        print("3. è‡ªå®šä¹‰èŒƒå›´çˆ¬å–")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
        
        if choice == "1":
            crawler.crawl_pages(1, 5)
        elif choice == "2":
            crawler.full_crawl()
        elif choice == "3":
            start = int(input("èµ·å§‹é¡µç : "))
            end = int(input("ç»“æŸé¡µç : "))
            crawler.crawl_pages(start, end)
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•çˆ¬å–")
            crawler.crawl_pages(1, 5)
    else:
        print("âš ï¸ è§†é¢‘ç±»å‹åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ç›´æ¥çˆ¬å–...")
        crawler.full_crawl()

if __name__ == '__main__':
    main()